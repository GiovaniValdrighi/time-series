---
title: "Modelling daily ozonio mean"
author: "Giovani Valdrighi, Vit√≥ria Guardieiro"
date: "30/09/2020"
output: powerpoint_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```



## Data

- New York data from 15/07/2001 to 30/04/2016.

```{r, message=FALSE}
library(ggplot2)
library(dplyr)
library(VIM)
library(zoo)
library(tseries)
library(randtests)
library(forecast)
library(car)

data <- read.csv("NY.csv")
data <- data %>% select(Date.Local, O3.Mean) %>% mutate(Date.Local = as.Date(Date.Local))
data <- data[-1, ]
startDate <- data$Date.Local[1] #First observed sunday
finalDate <- data$Date.Local[length(data$Date.Local)] #Last observed day
days <- seq(from = startDate, to = finalDate, by = 1)
dayWeek <- seq(from = startDate, to =  finalDate, by = 7)
#fulldata have NA rows in days that there is no observation
fulldata <- data.frame(Date.Local = days)
fulldata <- as.data.frame(merge(data, fulldata, all.y = TRUE)) %>% mutate(Date.Local = as.Date(Date.Local))
ggplot(data = fulldata) +
  geom_point(aes(x = Date.Local, y = O3.Mean)) +
  scale_x_date(date_breaks = "1 years", date_labels = "%Y", name = "Date") +
  scale_y_continuous(name = "O3") +
  ggtitle("Daily O3 mean") + 
  theme_bw()
```

## Missing data

- There are 52 time skips in the data, in a total of 473 days.
- The biggest skips is 108 days in 2011.
- The majority of skips are of 1 or 2 days.
- Around 9.5% missing data.
- The missing observations are distributed along the time without a clear pattern.


---

```{r}
dateJumps <- data$Date.Local - lag(data$Date.Local, n = 1)
dateJumps[is.na(dateJumps)] <- FALSE
ggplot(data = data[dateJumps > 1, ]) + 
  geom_point(aes(x = Date.Local, y = O3.Mean), colour = 'red') +
  scale_x_date(name = "Date") + 
  scale_y_continuous(name = "O3") +
  ggtitle("Observations after data skips") +
  theme_bw()
```

----

```{r}
skips <- dateJumps[dateJumps > 1]
ggplot(data = as.data.frame(skips)) +
  geom_bar(aes(skips), fill = "blue") +
  scale_y_continuous(name = "Count") +
  scale_x_continuous(name = "Quantity of days", breaks = seq(0, 150, 5)) +
  ggtitle("Days missing in sequence") +
  theme_bw()
```

## Imputation method

- It was used the kNN method to imputate values on missing observations.
- The kNN method needs the parameter k, the number of closest points considered.
- Starting with k = 7.

---

```{r}
imputedData <- data.frame("obs" = index(fulldata), "O3.Mean" = fulldata$O3.Mean)
imputedData <- kNN(imputedData, variable = "O3.Mean", k = 7) #K = 7 because we'll consider the week of the missing data
imputedData$Date.Local <- days

ggplot() +
  geom_point(data = imputedData[is.na(fulldata$O3.Mean), ], aes(x = Date.Local, y = O3.Mean), colour = 'red') +
  geom_point(data = fulldata, aes(x = Date.Local, y = O3.Mean), alpha = 0.2, fill = "black") +
  scale_y_continuous(name = "O3") +
  scale_x_date(date_breaks = "1 years", date_labels = "%Y", name = "Date") +
  ggtitle("Real vs Imputed data") +
  theme_bw()
```

---

- Method create a bad behavior where the size of the skips is bigger than 7 days.

```{r}
ggplot() +
  geom_point(data = imputedData[is.na(fulldata$O3.Mean), ], aes(x = Date.Local, y = O3.Mean), colour = 'red') +
  geom_point(data = fulldata, aes(x = Date.Local, y = O3.Mean), alpha = 0.2, fill = "black") +
  scale_y_continuous(name = "O3") +
  scale_x_date(date_breaks = "1 years", date_labels = "%Y", name = "Date", limits = c(as.Date("2011-01-01"), as.Date("2012-01-01"))) +
  ggtitle("Real vs Imputed data") +
  theme_bw()
```


---

- To deal with this, the parameter k used for imputation will be different if the size of the skip is minor tem 30 days, between 30 days and 100 days, or bigger than 100 days.
- k = 7, k = 45, k = 120, respectively.
- We will aggregate closest points by weighted by distance mean.

---

```{r}
imputedData[((is.na(fulldata$O3.Mean)) 
             & (fulldata$Date.Local > "2003-08-01") 
             & (fulldata$Date.Local < "2003-09-15")),"O3.Mean"] <- NA

imputedData[((is.na(fulldata$O3.Mean)) 
             & (fulldata$Date.Local > "2009-07-01") 
             & (fulldata$Date.Local < "2009-08-20")),"O3.Mean"] <- NA

imputedData <- kNN(imputedData, variable = c("O3.Mean"), numFun = weighted.mean, k = 45, weightDist = TRUE)


imputedData[((is.na(fulldata$O3.Mean)) 
              & (fulldata$Date.Local > "2011-01-01") 
              & (fulldata$Date.Local < "2012-01-01")),"O3.Mean"] <- NA

imputedData <- kNN(imputedData, variable = c("O3.Mean"), numFun = weighted.mean, k = 120, weightDist = TRUE)

ggplot() +
  geom_point(data = imputedData[is.na(fulldata$O3.Mean), ], aes(x = Date.Local, y = O3.Mean), colour = 'red') +
  geom_point(data = fulldata, aes(x = Date.Local, y = O3.Mean), alpha = 0.2, fill = "black") +
  scale_y_continuous(name = "O3") +
  scale_x_date(date_breaks = "1 years", date_labels = "%Y", name = "Date") +
  ggtitle("Real vs Imputed data (by separeted imput.)") +
  theme_bw()

```

---

```{r}
ggplot() +
  geom_point(data = imputedData[is.na(fulldata$O3.Mean), ], aes(x = Date.Local, y = O3.Mean), colour = 'red') +
  geom_point(data = fulldata, aes(x = Date.Local, y = O3.Mean), alpha = 0.2, fill = "black") +
  scale_y_continuous(name = "O3") +
  scale_x_date(date_breaks = "1 years", date_labels = "%Y", name = "Date", limits = c(as.Date("2011-01-01"), as.Date("2012-01-01"))) +
  ggtitle("Real vs Imputed data (by separated imput.)") +
  theme_bw()
```

## Weekly data
```{r}
k = 0
val = 1
l = rep(NA, length(fulldata$Date.Local) - 1)
while(k < length(fulldata$Date.Local) - 1){
  for(i in 1:7){
    l[k+i] = val
  }
  val = val + 1
  k = k +7
}

weekData <- cbind(fulldata)
weekData$weekInd <- l
weekData <- weekData %>% group_by(weekInd) %>% summarise(O3.Mean = mean(O3.Mean, na.rm = TRUE))
weekData$Date.Local <- dayWeek
ggplot(weekData) +
  geom_point(aes(x = Date.Local, y = O3.Mean)) +
  scale_y_continuous(name = "O3") +
  scale_x_date(date_breaks = "2 year", name = "Date", date_labels  = "%Y") + 
  ggtitle("Weekly O3 mean") + 
  theme_bw()
```


---

- If the data is grouped by week, ignoring the missing values when aggregating, it'll have 33 missing observations.
- Around 4.3% missing data.

```{r}

temp <- weekData[!is.na(weekData$O3.Mean), ]
skips <- temp$Date.Local - lag(temp$Date.Local)
skips <- skips[skips > 7][-1]/7

ggplot(data = as.data.frame(skips)) +
  geom_bar(aes(skips), fill = "blue") +
  scale_y_continuous(name = "Count") +
  scale_x_continuous(name = "Quantity of weeks", breaks = seq(0, 16, 2)) +
  ggtitle("Weeks missing in sequence") +
  theme_bw()
```

---

```{r}
imputedWeek <- cbind(weekData)
imputedWeek <- kNN(imputedWeek, variable = "O3.Mean", k = 4, 
                   numFun = weighted.mean, weightDist =  TRUE)

ggplot() +
  geom_point(data = imputedWeek[is.na(weekData$O3.Mean),], aes(x = Date.Local, y = O3.Mean), colour = 'red') +
  geom_point(data = weekData, aes(x = Date.Local, y = O3.Mean), colour = "black", alpha = 0.4) +
  scale_x_date(name ="Date", date_breaks = "1 year", date_labels = "%Y") +
  scale_y_continuous(name = "O3") + 
  ggtitle("Weekly Real vs Imputed data") +
  theme_bw()
```

---

- It has the same problem when the sequence of missing data is to big.
- Again, if there is more than 5 missing weeks, it will be used k = 16, if it's less, it'll be k = 4.

```{r}
imputedWeek[(imputedWeek$Date.Local > "2011-01-01") 
            & (imputedWeek$Date.Local < "2012-01-01") 
            & (is.na(weekData$O3.Mean)),"O3.Mean"] <- NA

imputedWeek <- kNN(imputedWeek, variable = "O3.Mean", k = 16, 
                   numFun = weighted.mean, weightDist =  TRUE)

ggplot() +
  geom_point(data = imputedWeek[is.na(weekData$O3.Mean), ], aes(x = Date.Local, y = O3.Mean), colour = 'red') +
  geom_point(data = weekData, aes(x = Date.Local, y = O3.Mean), alpha = 0.2, fill = "black") +
  scale_y_continuous(name = "O3") +
  scale_x_date(date_breaks = "1 years", date_labels = "%Y", name = "Date")+
  ggtitle("Weekly Real vs Imputed data") +
  theme_bw()
```

---

```{r}
ggplot() +
  geom_point(data = imputedWeek[is.na(weekData$O3.Mean), ], aes(x = Date.Local, y = O3.Mean), colour = 'red') +
  geom_point(data = weekData, aes(x = Date.Local, y = O3.Mean), alpha = 0.2, fill = "black") +
  scale_y_continuous(name = "O3") +
  scale_x_date(date_breaks = "1 years", date_labels = "%Y", name = "Date", limits = c(as.Date("2011-01-01"), as.Date("2012-01-01"))) +
  ggtitle("Weekly Real vs Imputed data") +
  theme_bw()
```

# Daily model

## Modelling process

- Metric to be minimized: MAE = $\dfrac{1}{n}\sum_n |y_t - \hat{y}_t|$.
- Rolling window of 2 years (730 days).
- Prediction of the next 7 ddays.
- First: Test if there is tendency with Wald-Wolfowitz runs test.
  - For every 2 years window, the p-value is smaller than $1e-3$.
- Second: Fitting of different models and evaluation of MAE error.

```{r}
trainDaily <- read.csv("trainDaily.csv")
trainDaily$Date.Local <- as.Date(trainDaily$Date.Local)
startDate <- trainDaily$Date.Local[1]
trainDailyZoo <- zoo(x = trainDaily$O3.Mean, order.by = trainDaily$Date.Local)
trainDaily.ts <- ts(data = trainDaily$O3.Mean, start = c(2001, as.numeric(format(startDate, "%j"))),
                   frequency = 365)
```

## Choice of models - trend

```{r}
plot(decompose(trainDaily.ts))
```

## Choice of models - ACF and PACF

```{r}
par(mfrow = c(1, 2))
acf(trainDaily.ts[1:730], main = "ACF on subset of train data")
pacf(trainDaily.ts[1:730], main = "PACF on subset of train data")
```

----

- Naive model: the next 7 days are predict as the mean of the last 4 weeks.
- Exponential smoothing forecast.
- Holt model with trend.
- ARMA(6,0) model.
- Auto ARIMA model.

----

- Process:
  - For each model:
    - For each 2 years window:
      - Fit model.
      - Generate predictions of next 7 days.
      - Compute mean of residuals for that window.
    - Compute MAE for model as the mean of residuals.
  
----

- Results for train data:
  - Auto ARIMA model: 0.005986731
  - Holt model: 0.006142857
  - SES model: 0.00617229
  - ARMA(6,0) model: 0.006279533
  - Naive model: 0.007498889

----

## Evaluating on test data

- MAE: 0.006503142

```{r}
testDaily <- read.csv("testDaily.csv")
testDaily$Date.Local <- as.Date(testDaily$Date.Local)
testDaily$month <- as.factor(format(testDaily$Date.Local, format = "%m"))
testDailyZoo <- zoo(x = testDaily$O3.Mean, order.by = testDaily$Date.Local)

hm_f <- function(x){
  holt_model <- holt(head(x,n=length(x)-7), h = 7)
  fore <- data.frame(forecast(holt_model))[, "Point.Forecast"]
  return(fore)
}

pred.holt <- rollapply(testDailyZoo, FUN = hm_f, width = 730, align = 'right')

mins <- apply(pred.holt, 1, min)
maxs <- apply(pred.holt, 1, max)

plot(tail(testDailyZoo, 366),  xlab = ("Date"), ylab = "O3 mean", main = "Holt model of O3 daily mean")
polygon(c(tail(testDaily$Date.Local,366), rev(tail(testDaily$Date.Local,366))),
        c(mins, maxs), col = rgb(0, 0, 0.8, 0.3))
lines(tail(testDaily$Date.Local,366),rowMeans(pred.holt), col = 'red')
legend("topright", legend=c("Predictions", "Real values", "Pred interval"), col=c('red','black', 'blue'), lty = 1:1, cex=0.8)
```

# Weekly model

## Modelling process

- Metric to be minimized: MAE = $\dfrac{1}{n}\sum_n |y_t - \hat{y}_t|$.
- Rolling window of 2 years (104 weeks), by skiping 4 weeks.
- Prediction of the next 4 weeks.
- First: Test if there is tendency with Wald-Wolfowitz runs test.
  - For every 2 years window, the p-value is smaller than $1e-3$.
- Second: Fitting of different models and evaluation of MAE error.

```{r}
trainWeekly <- read.csv("trainWeekly.csv")
trainWeekly$Date.Local <- as.Date(trainWeekly$Date.Local)
startDate <- trainWeekly$Date.Local[1]
trainWeekly.ts <- ts(data = trainWeekly$O3.Mean, start = c(2001, format(startDate, "%U")), frequency = 52)
tend <- function(x){
  p <- runs.test(x)
  p$p.value
}

tend.w <- rollapply(trainWeekly.ts, width = 104, FUN = tend, align = "left")
trainWeekly$month <- as.factor(format(trainWeekly$Date.Local, format = "%m"))
trainWeekly$ind <- seq(1, length(trainWeekly$Date.Local), 1)
```

## Choice of models - trend

```{r}
plot(decompose(trainWeekly.ts))
```

## Choice of models - ACF and PACF

```{r}
par(mfrow = c(1, 2))
acf(trainWeekly.ts[1:104], main = "ACF on subset of train data")
pacf(trainWeekly.ts[1:104], main = "PACF on subset of train data")
```

----

- Naive model: the next 4 weeks are predict as the mean of the last 4 weeks.
- Seasonal model: linear regression on seasonal dummies variable, each month is a factor.
- Linear model: linear regression on seasonal dummies and time index.
- Poly 2 model: linear regression on seasonal dummies and time index with degree 1 and 2.
- Poly 3 model: linear regression on seasonal dummies and time index with degree 1, 2, and 3.
- Holt model with trend.
- Holt Winters model with trend and seasonality (multiplicative and addtive).
- ARMA(1, 0) model.

----

- Process:
  - 1. For every 2 years window:
    - Fit all the models.
    - Generate predictions of next 4 weeks.
  - 2. With predictions for every week, compute residuals $r_t = y_t - \hat{y}_t$.
  - 3. With residuals, compute MAE.
  
----

- Results:
  - Sazonal: 0.003903618
  - Linear: 0.004032514
  - Poly 2: 0.004171369
  - Arma(1, 0): 0.004568415
  - Poly 3: 0.004739532
  - Holt: 0.004885386
  - HoltWinters additive: 0.005008383 
  - HoltWinters multiplicative: 0.005085043 
  - Naive: 0.005122260
  

```{r}
predictions <- data.frame()

for(i in seq(1, 508, 4)){
  #variables
  o3 <- trainWeekly$O3.Mean[i:(i+103)]
  startDate <- trainWeekly$Date.Local[i]
  o3.ts <- ts(o3, frequency = 52, start = c(format(startDate, "%Y"), format(startDate, "%U")))
  ind <- trainWeekly$ind[i:(i+103)]
  month <- trainWeekly$month[i:(i+103)]
  
  #models
  #linear regression
  model.sazonal <- lm(o3~month)
  model.sazonal_linear <- lm(o3~ind+month)
  model.sazonal_poly2 <- lm(o3~poly(ind, 2) + month)
  model.sazonal_poly3 <- lm(o3~poly(ind, 3) + month)
  
  #o3.test <- o3 - model.sazonal$fitted.values
  
  #param <- optim(c(0.1, 0.1), fn = optim_holt, gr = "L-BFGS-B")
  #holt method
  model.holt <- holt(o3.ts, h = 4)
  
  #holt winters
  model.hw_add <- HoltWinters(o3.ts, seasonal = "additive")
  model.hw_mul <- HoltWinters(o3.ts, seasonal = "mult")
  
  #arma
  model.arma <- arima(o3.ts, order = c(1, 0 ,0))
  #model.arma <- auto.arima(o3.ts)
  
  newdata = data.frame(ind = (i+104):(i+107), 
                        month = trainWeekly$month[(i+104): (i+107)])
  
  #prediction
  pred.naive <- rep(mean(o3[(length(o3)-4):length(o3)]), 4)
  pred.sazonal <- predict(model.sazonal, newdata = newdata)
  pred.linear <- predict(model.sazonal_linear, newdata = newdata)
  pred.poly_2 <- predict(model.sazonal_poly2, newdata = newdata)
  pred.poly_3 <- predict(model.sazonal_poly3, newdata = newdata)
  pred.holt <- coredata(model.holt$mean)
  pred.hw_add <- coredata(forecast(model.hw_add, h = 4)$mean)
  pred.hw_mul <- coredata(forecast(model.hw_mul, h = 4)$mean)
  pred.arma <- coredata(forecast(model.arma, h = 4)$mean)
  
  #updating dataframe of predictions  
  temp <- data.frame(Date.Local = trainWeekly$Date.Local[(i+104): (i+107)],
                     pred.naive = pred.naive,
                     pred.sazonal = pred.sazonal,
                     pred.linear = pred.linear,
                     pred.poly_2 = pred.poly_2,
                     pred.poly_3 = pred.poly_3,
                     pred.holt = pred.holt,
                     pred.hw_add = pred.hw_add,
                     pred.hw_mul = pred.hw_mul,
                     pred.arma = pred.arma,
                     real = trainWeekly$O3.Mean[(i+104): (i+107)])
  
  predictions <- rbind(predictions, temp)
}

#-------
#calculating residuals
residuals <- data.frame(naive = predictions$pred.naive - predictions$real,
                        sazonal = predictions$pred.sazonal - predictions$real,
                        linear = predictions$pred.linear - predictions$real,
                        poly_2 = predictions$pred.poly_2- predictions$real,
                        poly_3 = predictions$pred.poly_3- predictions$real,
                        holt = predictions$pred.holt- predictions$real,
                        hw_add = predictions$pred.hw_add- predictions$real,
                        hw_mul = predictions$pred.hw_mul - predictions$real,
                        arma_10 = predictions$pred.arma - predictions$real)
aux <- function(x){
  return(mean(abs(x)))
}
```

## Residuals

```{r}
acf(residuals$sazonal, main = "Sazonal model  residuals ACF")
```

----

```{r}
qqPlot(residuals$sazonal, main = "QQPlot of sazonal model residuals", ylab = "Residual")
```

----

```{r}
acf(residuals$linear, main = "Linear model  residuals ACF")
```

----

```{r}
qqPlot(residuals$linear, main = "QQPlot of linear model residuals", ylab = "Residual")
```


## Evaluating on test data

- MAE: 0.003438587
```{r}
testWeekly <- read.csv("testWeekly.csv")
testWeekly$Date.Local <- as.Date(testWeekly$Date.Local)
testWeekly$month <- as.factor(format(testWeekly$Date.Local, format = "%m"))
predictions.test <- data.frame()
for(i in seq(1, 52, 4)){
  #variables
  o3 <- testWeekly$O3.Mean[i:(i+103)]
  startDate <- testWeekly$Date.Local[i]
  o3.ts <- ts(o3, frequency = 52, start = c(format(startDate, "%Y"), format(startDate, "%U")))
  month <- testWeekly$month[i:(i+103)] 
  
  model.sazonal <- lm(o3~month)
  newdata = data.frame(month = testWeekly$month[(i+104): (i+107)])
  pred <- forecast(model.sazonal, newdata = newdata)
  temp <- data.frame(Date.Local = testWeekly$Date.Local[(i+104): (i+107)],
                     pred = pred$mean,
                     low = pred$lower[,2],
                     up = pred$upper[,2],
                     real = testWeekly$O3.Mean[(i+104): (i+107)])
  predictions.test <- rbind(predictions.test, temp)
  
}
residuals.test <- predictions.test$pred - predictions.test$real

startDate <- as.Date("2015-05-03")
endDate <- as.Date("2016-04-24")

plot(testWeekly$Date.Local, testWeekly$O3.Mean, type = 'l',  xlab = ("Date"), ylab = "O3 mean",
     xlim = c(startDate, endDate), main = "Sazonal model of O3 weekly mean")
polygon(c(predictions.test$Date.Local, rev(predictions.test$Date.Local)),
        c(predictions.test$low, rev(predictions.test$up)), col = rgb(0, 0, 0.8, 0.3))
lines(testWeekly$Date.Local, testWeekly$O3.Mean)
lines(predictions.test$Date.Local, predictions.test$pred, col = 'red')
```
